{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd019a1e7a781baaae921377895d6e510fae92780594430376723c234223caeb85b",
   "display_name": "Python 3.8.3 64-bit ('ahmet': virtualenv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "8e61c9422603e3d3820d3aeca1b8eefe471d1a1023a28a3903412dc32402bf43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## EEE485 - Project Final"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import smote\n",
    "import random\n",
    "import kNN\n",
    "import fcn\n",
    "import PCA"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Load Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"data.csv\")\n",
    "bankrupt_pd = raw_data[\"Bankrupt?\"]\n",
    "features_pd = raw_data.drop([\"Bankrupt?\"], axis=1)\n",
    "raw_data"
   ]
  },
  {
   "source": [
    "### Check for NAN and Duplicate Values\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NAN values:\", [col for col in features_pd if features_pd[col].isna().sum() > 0])\n",
    "print(\"Duplicates:\", features_pd.duplicated().sum())"
   ]
  },
  {
   "source": [
    "We now know that we do not have any missing or duplicate data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Evaluate Data Imbalance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstable_initial = (raw_data[\"Bankrupt?\"] == 1).sum()\n",
    "stable_initial = (raw_data[\"Bankrupt?\"] == 0).sum()\n",
    "print(\"Data Size:\", raw_data.shape[0])\n",
    "print(\"# of stable companies:\", stable_initial )\n",
    "print(\"# of unstable companies:\", unstable_initial )\n",
    "print(\"Unstable to Stable Ratio: \", unstable_initial/stable_initial)"
   ]
  },
  {
   "source": [
    "### PLOTS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.hist(figsize = (50,40), bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(ncols=4, figsize = (24,6) )\n",
    "\n",
    "sns.boxplot(x=\"Bankrupt?\", y=\" Cash/Total Assets\", data=raw_data, ax = axes[0] )\n",
    "axes[0].set_title(\"Bankrupt vs Cash/Total Assets\")\n",
    "\n",
    "sns.boxplot(x=\"Bankrupt?\", y=\" Current Assets/Total Assets\", data=raw_data, ax = axes[1] )\n",
    "axes[1].set_title(\"Bankrupt vs Current Assets/Total Assets\")\n",
    "\n",
    "sns.boxplot(x=\"Bankrupt?\", y=\" Net worth/Assets\", data=raw_data, ax = axes[2] )\n",
    "axes[2].set_title(\"Bankrupt vs Net worth/Assets\")\n",
    "\n",
    "sns.boxplot(x=\"Bankrupt?\", y=\" Cash/Current Liability\", data=raw_data, ax = axes[3] )\n",
    "axes[3].set_title(\"Bankrupt vs Cash/Current Liability\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Outlier Removal Using IQR"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = raw_data.copy(deep=True)\n",
    "for col in features_pd:\n",
    "    clean_data = fcn.remove_outlier(raw_data[col], str(col), raw_data)\n",
    "clean_data = clean_data.reset_index(drop=True)\n",
    "clean_data"
   ]
  },
  {
   "source": [
    "### Plots with Outliers Removed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.hist(figsize = (50,40), bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(ncols=4, figsize = (24,6) )\n",
    "\n",
    "sns.boxplot(x=\"Bankrupt?\", y=\" Cash/Total Assets\", data=clean_data, ax = axes[0] )\n",
    "axes[0].set_title(\"Bankrupt vs Cash/Total Assets\")\n",
    "\n",
    "sns.boxplot(x=\"Bankrupt?\", y=\" Current Assets/Total Assets\", data=clean_data, ax = axes[1] )\n",
    "axes[1].set_title(\"Bankrupt vs Current Assets/Total Assets\")\n",
    "\n",
    "sns.boxplot(x=\"Bankrupt?\", y=\" Net worth/Assets\", data=clean_data, ax = axes[2] )\n",
    "axes[2].set_title(\"Bankrupt vs Net worth/Assets\")\n",
    "\n",
    "sns.boxplot(x=\"Bankrupt?\", y=\" Cash/Current Liability\", data=clean_data, ax = axes[3] )\n",
    "axes[3].set_title(\"Bankrupt vs Cash/Current Liability\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "\n",
    "class PCAnalyser():\n",
    "\n",
    "    def __init__(self, X: np.ndarray, data_centered=False) -> None:\n",
    "        if data_centered:\n",
    "            self.X = X\n",
    "        else:\n",
    "            self.X = X - np.mean(X, axis=0)\n",
    "        self.Sigma = self.X.T @ self.X\n",
    "        self.eigs = np.array([])\n",
    "        return\n",
    "\n",
    "    def analyse(self, k = 10):\n",
    "        if k > self.Sigma.shape[0]: return\n",
    "\n",
    "        eig_vals, eig_vecs = LA.eigh(self.Sigma)\n",
    "        idx = np.argsort(eig_vals)[::-1]\n",
    "        self.eigs = eig_vals[idx]\n",
    "        eig_vecs = eig_vecs[:,idx]\n",
    "        PCs = eig_vecs[:,0:k]\n",
    "\n",
    "        return self.eigs, PCs\n",
    "\n",
    "    def calc_PVE(self, m=10, individual=False):\n",
    "        m = np.clip(m, 0, len(self.eigs))\n",
    "        if individual:\n",
    "            return self.eigs[m] / sum(self.eigs) # PVE(m)\n",
    "        return sum(self.eigs[:m+1]) / sum(self.eigs) # PVE(first m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_X = clean_data.drop([\"Bankrupt?\"], axis=1)\n",
    "clean_Y = clean_data[\"Bankrupt?\"]\n",
    "centered_data = clean_X - np.mean(clean_X, axis=0)\n",
    "pc_analyser = PCAnalyser(centered_data, data_centered=True)\n",
    "eigen_vals, PCs = pc_analyser.analyse(k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(eigen_vals)\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Eigen Value\")\n",
    "plt.title(\"Eigen Values of the Principal Components\")\n",
    "plt.xlim( (0, 30) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_data = centered_data @ PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_data.columns = (\"PC\"+str(i) for i in range(1,9))\n",
    "print(\"Shape of the Feature Matrix after PCA is:\", PCA_data.shape)\n",
    "PCA_data = pd.concat([clean_Y, PCA_data], axis=1)"
   ]
  },
  {
   "source": [
    "### SMOTE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority = PCA_data[PCA_data[\"Bankrupt?\"] == 1] # Extract minority samples from data\n",
    "smt = smote.Smote( minority.to_numpy() ) # Initialize the SMOTE class\n",
    "oversamples = smt.oversample(N=2600) # Employ SMOTE oversampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_data = PCA_data.copy(deep=True) # Cleared from outliers and dim reduced by PCA. Now oversample\n",
    "oversamples_pd = pd.DataFrame(oversamples, columns = PCA_data.columns)\n",
    "smote_data = smote_data.append(oversamples_pd)\n",
    "smote_data = smote_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstable_smote = (smote_data[\"Bankrupt?\"] == 1).sum()\n",
    "stable_smote = (smote_data[\"Bankrupt?\"] == 0).sum()\n",
    "print(\"Oversampled Data Size:\", smote_data.shape[0])\n",
    "print(\"Number of Stable Companies:\", stable_smote)\n",
    "print(\"Number of Unstable Companies (with SMOTE):\", unstable_smote)\n",
    "print(\"unstable to Stable Ratio: \", unstable_smote/stable_smote, sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_data[\"Bankrupt?\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(ncols=4, figsize = (24,6) )\n",
    "\n",
    "sns.boxplot(x=\"Bankrupt?\", y=\" Cash/Total Assets\", data=smote_data, ax = axes[0] )\n",
    "axes[0].set_title(\"Bankrupt vs Cash/Total Assets\")\n",
    "\n",
    "sns.boxplot(x=\"Bankrupt?\", y=\" Current Assets/Total Assets\", data=smote_data, ax = axes[1] )\n",
    "axes[1].set_title(\"Bankrupt vs Current Assets/Total Assets\")\n",
    "\n",
    "sns.boxplot(x=\"Bankrupt?\", y=\" Net worth/Assets\", data=smote_data, ax = axes[2] )\n",
    "axes[2].set_title(\"Bankrupt vs Net worth/Assets\")\n",
    "\n",
    "sns.boxplot(x=\"Bankrupt?\", y=\" Cash/Current Liability\", data=smote_data, ax = axes[3] )\n",
    "axes[3].set_title(\"Bankrupt vs Cash/Current Liability\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Test Train Split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.1\n",
    "#Smote\n",
    "train_sm, test_sm = fcn.test_train_split(smote_data, test_ratio )\n",
    "X_train_sm = train_sm.drop([\"Bankrupt?\"], axis=1)\n",
    "Y_train_sm = train_sm[\"Bankrupt?\"]\n",
    "X_test_sm = test_sm.drop([\"Bankrupt?\"], axis=1)\n",
    "Y_test_sm = test_sm[\"Bankrupt?\"]\n",
    "#No Smote\n",
    "train, test = fcn.test_train_split(clean_data, test_ratio )\n",
    "X_train = train.drop([\"Bankrupt?\"], axis=1)\n",
    "Y_train = train[\"Bankrupt?\"]\n",
    "X_test = test.drop([\"Bankrupt?\"], axis=1)\n",
    "Y_test = test[\"Bankrupt?\"]\n"
   ]
  },
  {
   "source": [
    "### k-Nearest Neighbors Classifier (with and without SMOTE)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = kNN.k_NN_classifier(X_train.to_numpy(), Y_train.to_numpy() )\n",
    "Y_test_np = Y_test.to_numpy()\n",
    "X_test_np = X_test.to_numpy()\n",
    "knn_preds = np.zeros_like(Y_test_np)\n",
    "for idx, test in enumerate(X_test_np):\n",
    "    knn_preds[idx] = knn_classifier.classify(test)\n",
    "\n",
    "knn_classifier_sm = kNN.k_NN_classifier(X_train_sm.to_numpy(), Y_train_sm.to_numpy() )\n",
    "Y_test_sm_np = Y_test_sm.to_numpy()\n",
    "X_test_sm_np = X_test_sm.to_numpy()\n",
    "knn_sm_preds = np.zeros_like(Y_test_sm_np)\n",
    "for idx, test in enumerate(X_test_sm_np):\n",
    "    knn_sm_preds[idx] = knn_classifier_sm.classify(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion Matrix Without SMOTE\n",
      " \t1\t0 (prediction)\n",
      "1\t0\t20\n",
      "0\t0\t607\n",
      "Recall: nan\n",
      "Precision: 0.0\n",
      "\n",
      "Confusion Matrix With SMOTE\n",
      " \t1\t0 (prediction)\n",
      "1\t552\t30\n",
      "0\t87\t524\n",
      "Recall: 86.3849765258216\n",
      "Precision: 94.84536082474226\n",
      "<ipython-input-132-5c16df14f5d7>:4: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  print(\"Recall:\", conf_matrix[0,0]/(conf_matrix[0,0]+conf_matrix[1,0]) * 100 )\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix Without SMOTE\")\n",
    "conf_matrix = fcn.confusion_matrix(Y_test_np, knn_preds, ret = True)\n",
    "\n",
    "print(\"Recall:\", conf_matrix[0,0]/(conf_matrix[0,0]+conf_matrix[1,0]) * 100 )\n",
    "print(\"Precision:\", conf_matrix[0,0]/(conf_matrix[0,0]+conf_matrix[0,1]) * 100 )\n",
    "print()\n",
    "print(\"Confusion Matrix With SMOTE\")\n",
    "conf_matrix_sm = fcn.confusion_matrix(Y_test_sm_np, knn_sm_preds, ret = True)\n",
    "print(\"Recall:\", conf_matrix_sm[0,0]/(conf_matrix_sm[0,0]+conf_matrix_sm[1,0]) * 100 )\n",
    "print(\"Precision:\", conf_matrix_sm[0,0]/(conf_matrix_sm[0,0]+conf_matrix_sm[0,1]) *100 )"
   ]
  },
  {
   "source": [
    "### Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy():\n",
    "    def __init__(self):\n",
    "         pass\n",
    "\n",
    "    def loss(self, y, p):\n",
    "        # Avoid division by zero\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
    "\n",
    "    def acc(self, y, p):\n",
    "        return accuracy_score(np.argmax(y, axis=1), np.argmax(p, axis=1))\n",
    "\n",
    "    def gradient(self, y, p):\n",
    "        # Avoid division by zero\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return - (y / p) + (1 - y) / (1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostClassify():\n",
    "\n",
    "    def __init__(self, tree_count, lr, tree_min_split, tree_min_impurity, tree_max_depth):\n",
    "        self.tree_count = tree_count\n",
    "        self.lr = lr\n",
    "        self.tree_min_split = tree_min_split\n",
    "        self.tree_min_impurity = tree_min_impurity\n",
    "        self.tree_max_depth = tree_max_depth\n",
    "        self.loss = CrossEntropy()\n",
    "\n",
    "        self.trees = []\n",
    "        for i in range(n)\n"
   ]
  },
  {
   "source": [
    "### Gradient Boosting Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import progressbar\n",
    "\n",
    "# Import helper functions\n",
    "from mlfromscratch.utils import train_test_split, standardize, to_categorical\n",
    "from mlfromscratch.utils import mean_squared_error, accuracy_score\n",
    "from mlfromscratch.deep_learning.loss_functions import SquareLoss, CrossEntropy\n",
    "from mlfromscratch.supervised_learning.decision_tree import RegressionTree\n",
    "from mlfromscratch.utils.misc import bar_widgets\n",
    "\n",
    "\n",
    "class GradientBoosting(object):\n",
    "    \"\"\"Super class of GradientBoostingClassifier and GradientBoostinRegressor. \n",
    "    Uses a collection of regression trees that trains on predicting the gradient\n",
    "    of the loss function. \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_estimators: int\n",
    "        The number of classification trees that are used.\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    min_samples_split: int\n",
    "        The minimum number of samples needed to make a split when building a tree.\n",
    "    min_impurity: float\n",
    "        The minimum impurity required to split the tree further. \n",
    "    max_depth: int\n",
    "        The maximum depth of a tree.\n",
    "    regression: boolean\n",
    "        True or false depending on if we're doing regression or classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators, learning_rate, min_samples_split,\n",
    "                 min_impurity, max_depth, regression):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity = min_impurity\n",
    "        self.max_depth = max_depth\n",
    "        self.regression = regression\n",
    "        self.bar = progressbar.ProgressBar(widgets=bar_widgets)\n",
    "        \n",
    "        # Square loss for regression\n",
    "        # Log loss for classification\n",
    "        self.loss = SquareLoss()\n",
    "        if not self.regression:\n",
    "            self.loss = CrossEntropy()\n",
    "\n",
    "        # Initialize regression trees\n",
    "        self.trees = []\n",
    "        for _ in range(n_estimators):\n",
    "            tree = RegressionTree(\n",
    "                    min_samples_split=self.min_samples_split,\n",
    "                    min_impurity=min_impurity,\n",
    "                    max_depth=self.max_depth)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.full(np.shape(y), np.mean(y, axis=0))\n",
    "        for i in self.bar(range(self.n_estimators)):\n",
    "            gradient = self.loss.gradient(y, y_pred)\n",
    "            self.trees[i].fit(X, gradient)\n",
    "            update = self.trees[i].predict(X)\n",
    "            # Update y prediction\n",
    "            y_pred -= np.multiply(self.learning_rate, update)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.array([])\n",
    "        # Make predictions\n",
    "        for tree in self.trees:\n",
    "            update = tree.predict(X)\n",
    "            update = np.multiply(self.learning_rate, update)\n",
    "            y_pred = -update if not y_pred.any() else y_pred - update\n",
    "\n",
    "        if not self.regression:\n",
    "            # Turn into probability distribution\n",
    "            y_pred = np.exp(y_pred) / np.expand_dims(np.sum(np.exp(y_pred), axis=1), axis=1)\n",
    "            # Set label to the value that maximizes probability\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class GradientBoostingRegressor(GradientBoosting):\n",
    "    def __init__(self, n_estimators=200, learning_rate=0.5, min_samples_split=2,\n",
    "                 min_var_red=1e-7, max_depth=4, debug=False):\n",
    "        super(GradientBoostingRegressor, self).__init__(n_estimators=n_estimators, \n",
    "            learning_rate=learning_rate, \n",
    "            min_samples_split=min_samples_split, \n",
    "            min_impurity=min_var_red,\n",
    "            max_depth=max_depth,\n",
    "            regression=True)\n",
    "\n",
    "class GradientBoostingClassifier(GradientBoosting):\n",
    "    def __init__(self, n_estimators=200, learning_rate=.5, min_samples_split=2,\n",
    "                 min_info_gain=1e-7, max_depth=2, debug=False):\n",
    "        super(GradientBoostingClassifier, self).__init__(n_estimators=n_estimators, \n",
    "            learning_rate=learning_rate, \n",
    "            min_samples_split=min_samples_split, \n",
    "            min_impurity=min_info_gain,\n",
    "            max_depth=max_depth,\n",
    "            regression=False)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = to_categorical(y)\n",
    "        super(GradientBoostingClassifier, self).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}